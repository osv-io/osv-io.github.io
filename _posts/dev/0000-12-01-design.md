---
layout: nav
title: Design
category: dev
show_heading: yes
nav: dev
---

## Virtual Memory

OS<sup>V</sup> is different than traditional systems in two respects:

First, it doesn't have multiple address spaces. In an ordinary system, virtual address 0x200000 may be mapped to different physical pages (or the same page, sometimes, or to no page), depending on the process. In osv an address is mapped to just one page. This means we don't have to switch mappings when the processor stops running one process and starts running another.

The second difference is that it doesn't maintain different permissions for the application and kernel. In an ordinary system, the kernel does not allow the the application to access kernel pages. This means it has to switch privilege levels when running kernel code, then switch back. OSV does not do this (expensive) switch; instead it relies on the JVM to block accesses to kernel memory.

<!--more-->

&nbsp;

1. All user threads share a memory address space (i.e., they are threads, not processes). This allowed simpler code, and better performance, as we can avoid a page-table switches involving a TLB flush (since Intel does not have a tagged TLB) that could occur when switching between processes.

2. There is no protection between the user-space and kernel - calling the kernel is just making a function call, and user-space and the kernel can read and write each other's memory. This brings performance benefits, and a lot of flexibility. But it means that even if we did have separate address space for different processes, they would not have UNIX-style isolation, because each process could "take over" the kernel (e.g., read temporary kernel structures that belong to the other process).

### Java Virtual Machine integration

Since the page table hardware is not used for inter-process and kernel/user isolation, OS<sup>V</sup> is free to use it for other purposes. The primary consumer of these resources is intended to be the Java Virtual Machine, which we will modifed to make use of them.

&nbsp;

Examples of areas where exposing the page table hardware can be used to improve the JVM:

&nbsp;

1. The JVM often needs to track writes to memory, in order to minimize heap scans during garbage collection. If it can determine that a memory area has not been modified, it does not need to scan it, reducing garbage collection time.<br/>
Currently, this is done using “cards”, or bitmaps that are managed by software – every write to memory also contains code to write to the appropriate card.<br/>
However, modern processor hardware already includes the ability to track writes – the Dirty bit in the page table entry (PTE) is automatically set whenever a memory page is written to. By allowing the JVM to query and reset the dirty bit, it can eliminate the code it generates to write into cards.

&nbsp;

2. The JVM sometimes needs to move memory around during garbage collection. This is slow, since it requires loading a lot of data into the processor's data cache, and then writing it out again – consuming a lot of memory bandwidth and wiping out recently-used data in the cache. With OSV however we can manipulate page-table entries to point at different physical pages, thereby “moving” them with almost zero cost.<br/>
These modifications are planned and have not been implemented yet.

### Shell

The OS<sup>V</sup> shell is based on CRaSH, with OSV specific extensions. The use of CRaSH allow rapidly developing new commands in the Groovy language (which is compatible with JVM-based APIs).

&nbsp;

&nbsp;

## Performance & Tracing

Performance measurements are based on static tracepoints, inserted into the code at strategic points. There are tools available from the shell to capture statistics on tracepoint hits, gather call-stack histograms on selected tracepoints, or simply capture a trace for later analysis.

Instructions: [https://github.com/cloudius-systems/osv/wiki/Debugging-OSv](https://github.com/cloudius-systems/osv/wiki/Debugging-OSv)

&nbsp;

Fairness



Requirements:

 - global fairness
 - cheap to compute

&nbsp;

Each task has a measure of runtime it receives from the scheduler. We could use the task’s total runtime, t, but we would like to consider only the recent history so that a task that has slept for a long time would not dominate the processor.  So we use a decaying average:

&nbsp;

R(t + t) =(1 - kt)R(t)  + p(t)r(t)kt(1)

&nbsp;

Where

- R is a measure of the runtime the thread has received in the recent past

- p is the thread’s normalized priority

- r describes the runtime history; it has the value 1 when the thread is running, and 0 otherwise

- k is a constant determining how soon the scheduler “forgets” a thread’s history, typical value is 50 1/s (equivalent to forgetting history in a few multiples of 20 ms).

- t is a small period

&nbsp;

Since we don’t want to have periodic scheduler ticks, we take the limit t0and integrate. This yields the normal continuous decaying average function:

&nbsp;

Rt0=0t0p(t)r(t)ek(t-t0)dt(2)

&nbsp;

The scheduler picks the runnable thread with smallest R, and runs it until some other thread has a smaller R.  To prevent excessive context switches, hysteresis is employed.

&nbsp;

Computing R continuously is expensive, but not needed.  A little algebra shows how to compute R(t2) given  R(t1), provided p(t) and r(t) have not changed between t1 and t2. This means we can update R only when the scheduler is invoked, or when priorities change, since r(t) only changes as a result of scheduler execution.

&nbsp;

R(t2) = ek(t1-t2)R(t1) + 1kp(t2)r(t2)(1-ek(t1-t2))(3)

&nbsp;

It is still impractical to compute R for all threads on every scheduler invocation; but we note that, for a given processor, r(t2)has the value 1 for at most one process; the others are not running (queued or sleeping). The second term of the equation vanishes; and the first is a multiplication by a constant (across all threads, for a given t2).

&nbsp;

Let us introduce an unnormalized runtime measure, R’. This measure is local to a processor; when moving a thread to a different processor, we must normalize it again:

&nbsp;

R'(t) = c(t)R(t)(4)

&nbsp;

If we define

&nbsp;

c(t2) =ek(t1-t2)c(t1) (5)

&nbsp;

Then the unnormalized runtime measure is given by

&nbsp;

R'(t2) = R'(t1) + 1kp(t2)r(t2)(e-k(t1-t2)-1)(6)

&nbsp;

For non-running threads, r(t2) = 0, so:

&nbsp;

R'(t2) = R'(t1)(7)

&nbsp;

So on each scheduler invocation, we only need to update c and R’ for the running thread.

&nbsp;

The value of c starts at 1 and decays towards zero; to avoid underflow we need to renormalize R’ periodically, by multiplying it (for all threads) by the current value of c, and then setting c to 1. This is done rarely enough so that the cost is amortized.

&nbsp;

Runnable threads are stored in a sorted container, with R’ as the key. When a thread is run, it is taken out of the container, and has its R’ updated when it is stopped. When a thread is migrated, R’ is normalized first (in the cpu it was running on) and then unnormalized again (in the cpu it is migrated to).

&nbsp;

To achieve hysteresis, the scheduler reduces the running thread’s R value by a constant tgranwhen it starts running the thread, and increases it back by the same amount when it stops running. This prevents the thread from being preempted immediately by a thread with the same or similar R value, yet preserves fairness.

&nbsp;

When a runnable (but not running) thread (denoted ‘q’) becomes the one with the lowest R value, the scheduler computes the tsin which it would have an R value lower than the running thread (denoted ‘r’):

&nbsp;

Rq(ts) = ek(t0 - ts)Rq(t0)

Rr(ts) = ek(t0 - ts)Rr(t0) +1kpr(ts)(1-ek(t1-t2))

&nbsp;

Setting Rq(ts) = Rr(ts) and solving for ts, we obtain

&nbsp;

ts =t0 - 1kln1kpr(t0)1kpr(t0) + Rq(t0) - Rr(t0)

&nbsp;

Adjusting for the unnormalized R value:

&nbsp;

ts =t0 - 1kln1kpr(t0)1kpr(t0) +Rq(t0) - Rr(t0)c(t0)

&nbsp;

The scheduler sets a timer at tswhich is the next preemption point whenever one of the parameters in the equation changes - the runnable thread, or the priority of the running thread. A small overshoot is allowable, so approximations can be used.

&nbsp;

The exponential function is easier to approximate in the form of a power-of-two function, so (6) becomes

&nbsp;

R'(t2) = R'(t1) + 1kp(t2)r(t2)(2-(log2e)k(t1-t2)-1)